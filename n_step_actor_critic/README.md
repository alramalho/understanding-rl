# One step advantage actor Critic

`agent` provides an agent only for discrete environments. \
An experiment is being done under `experiment.py` where AC is trying to be used in continuous environments using
`MultivariateNormal` dist instead of `Categorical` (similar to PPO)

## TODO
- finish experiment. If it is impossible document here why.

## pseudocode
```
# INITIALISE EXPERIMENT
actor_Î¸, critic_w â† parameterised functions instantiated as neural networks
É‘_Î¸, É‘_w â† step size for actor and critic respectively

# EXPERIMENT LOOP
for each episode in max_episodes:
    
    # INITIALISE EPISODE
    state â† start state of environment
		ğš¿_record, log_probabilities, states, rewards â† []
    
    # EPISODE LOOP
    for each step in episode t=0,1,...,T:
        policy â† actor_Î¸(state)
        log_probabilities â† log(policy)
        next_state, reward â† environment_step(sample(policy))
        rewards â† rewards + reward
        states â† states + state
        ğš¿_record â† ğš¿_record + ğš¿_function(next_state, states, rewards, critic)
        state = next_state
    endfor
   
   	# UPDATE PARAMETERS
   	loss_actor â† mean( - log_probrobabilities * ğš¿  									  																		- log_probabilities.entropy() * 0.01) # polict gradient
    loss_critic â† mean(ğš¿**2) 																# mean squared error
    Î¸ â† Î¸ + É‘_Î¸ * loss_actor
    w â† w + É‘_w * loss_actor

endfor
```


where 
![img.png](../_imgs/ac_helper.png)


taken from [here](https://github.com/alex-lindt/Variance-N-Step-Actor-Critic) - don't love it but not in the mood to mess with latex ğŸ¤·â€â™‚ï¸ to draft something fancier