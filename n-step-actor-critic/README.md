# One step advantage actor Critic

## pseudocode
```
# INITIALISE EXPERIMENT
actor_Î¸, critic_w â† parameterised functions instantiated as neural networks
É‘_Î¸, É‘_w â† step size for actor and critic respectively

# EXPERIMENT LOOP
for each episode in max_episodes:
    
    # INITIALISE EPISODE
    state â† start state of environment
		ğš¿_record, log_probabilities, states, rewards â† []
    
    # EPISODE LOOP
    for each step in episode t=0,1,...,T:
        policy â† actor_Î¸(state)
        log_probabilities â† log(policy)
        next_state, reward â† environment_step(sample(policy))
        rewards â† rewards + reward
        states â† states + state
        ğš¿_record â† ğš¿_record + ğš¿_function(next_state, states, rewards, critic)
        state = next_state
    endfor
   
   	# UPDATE PARAMETERS
   	loss_actor â† mean( - log_probrobabilities * ğš¿  									  																		- log_probabilities.entropy() * 0.01) # polict gradient
    loss_critic â† mean(ğš¿**2) 																# mean squared error
    Î¸ â† Î¸ + É‘_Î¸ * loss_actor
    w â† w + É‘_w * loss_actor

endfor
```


where 
![img.png](../_imgs/ac_helper.png)
